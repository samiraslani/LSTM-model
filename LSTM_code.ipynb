{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6b1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dec442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(y): \n",
    "    '''\n",
    "    This function maps the probabilities outputed by the model back to the rankings list \n",
    "    and outputs the ranking with the highest probability. \n",
    "    \n",
    "    inputs: \n",
    "    y  (1,m)     : Probability output of the RNN model \n",
    "    \n",
    "    outputs: \n",
    "    res (string) : The ranking corresponding to the most probable outcome. \n",
    "    \n",
    "    '''\n",
    "    y = y.tolist()\n",
    "    #ranking = ['Below Average' , 'Average' , 'Above Average']\n",
    "    res = ranking[y.index(max(y))]\n",
    "    return(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecf5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to write another function that maps the correct output of the function to the rankings. \n",
    "def vec_output(y): \n",
    "    m = len(ranking)\n",
    "    txt = y\n",
    "    v = np.zeros(m) \n",
    "    j = ranking.index(txt)\n",
    "    v[j] = 1\n",
    "    return v \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381f7f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_actual):\n",
    "    \n",
    "    accuracy = 0\n",
    "    right_pred = np.zeros(len(y))\n",
    "    for i in range(0,len(y)): \n",
    "        right_pred[i] = pred(y[i]) == y_actual[i]\n",
    "    \n",
    "    accuracy = sum(right_pred)/len(y) * 100\n",
    "    accuracy = format(accuracy, \".2f\")  \n",
    "    #print(f\"The accuracy of the model is {accuracy}%\")\n",
    "    return(accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61606ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               reviewId            userName  \\\n",
      "0     gp:AOqpTOFxf3fttcT5DSvFIn9KPp5FErgH9yC533Fmoxv...      Donna Caritero   \n",
      "1     gp:AOqpTOEq6rNIWLnPV4KFTctWvm0mpGEQljtD6mvy1H-...  Soumi Mukhopadhyay   \n",
      "2     gp:AOqpTOE86hSyPRHZgYt28Uk5zGe4FZGb1hkmtFDiYJ2...   Theknown _unknown   \n",
      "3     gp:AOqpTOHSuKkVTcM3QgCCKysHQlxEnk2ocOKsUMiMIJy...        Anthony Dean   \n",
      "4     gp:AOqpTOEOrZt5H6jXPiplJyffCd5ZBnVXACTWgwNsF1R...   Neha Diana Wesley   \n",
      "...                                                 ...                 ...   \n",
      "1495  gp:AOqpTOHhnXMpylU3f-1V1KbR2hwWArOilxPlKI6K4xY...            Reen Ali   \n",
      "1496  gp:AOqpTOEcz62DHS-amqTB5xGMhM4_R0UJpcv_HDNny9i...     Shaurya Chilwal   \n",
      "1497  gp:AOqpTOFMqEqa_kpp29Q8wjcBmKUCAvOQGQx4KZQ8b83...           GK Gaming   \n",
      "1498  gp:AOqpTOGY4z3pUxeiqGzn2ad3Noxqlbm-9DZ3ksHqD1_...    1203_Vani Sharma   \n",
      "1499  gp:AOqpTOFVGZ0MXyR-Gv_d2cYf2KD709Hwple_u7OZE4y...           MeLLy EcK   \n",
      "\n",
      "                                              userImage  \\\n",
      "0     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "2     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "3     https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
      "4     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "...                                                 ...   \n",
      "1495  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1496  https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
      "1497  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1498  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1499  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "\n",
      "                                                 review  score  thumbsUpCount  \\\n",
      "0     Overall it's really an amazing app. I've been ...      4            528   \n",
      "1     Hey! Yes I gave a 5 star rating... coz I belie...      5            351   \n",
      "2     Canva used to be a good app! But recently I've...      1            160   \n",
      "3     It's a brilliant app, but I have just one prob...      5            145   \n",
      "4     This was such a great app. I used to make BTS ...      4            142   \n",
      "...                                                 ...    ...            ...   \n",
      "1495   Absolutely amazing and a lifesaver for teachers.      5              0   \n",
      "1496  Very nice but many a times shows error opening...      3              0   \n",
      "1497  Too much lag. Always stuck on starting page. W...      1              0   \n",
      "1498  Nice app for all college work. So many feature...      5              0   \n",
      "1499  I am a teacher and this was so much for this s...      5              0   \n",
      "\n",
      "     reviewCreatedVersion                  at  \\\n",
      "0                 2.116.0 2021-06-17 07:18:54   \n",
      "1                 2.116.0 2021-06-17 19:18:28   \n",
      "2                 2.116.0 2021-06-23 19:13:28   \n",
      "3                 2.116.0 2021-06-19 23:36:07   \n",
      "4                 2.116.0 2021-06-21 00:29:27   \n",
      "...                   ...                 ...   \n",
      "1495              2.127.0 2021-09-07 02:45:51   \n",
      "1496              2.123.0 2021-08-14 07:36:50   \n",
      "1497              2.118.0 2021-07-06 03:34:38   \n",
      "1498              2.123.0 2021-08-07 18:47:15   \n",
      "1499              2.117.0 2021-06-28 03:40:57   \n",
      "\n",
      "                                           replyContent           repliedAt  \\\n",
      "0     Hi Donna. We are sorry that your text or desig... 2021-06-19 21:24:32   \n",
      "1                                                   NaN                 NaT   \n",
      "2     Hi there. We're sorry to hear that you are hav... 2021-06-26 20:20:56   \n",
      "3                                                   NaN                 NaT   \n",
      "4     Hi Neha. Sorry to hear that you are unable to ... 2021-06-24 20:46:28   \n",
      "...                                                 ...                 ...   \n",
      "1495                                                NaN                 NaT   \n",
      "1496  Hi Shaurya, we're sorry if the app shows error... 2021-08-22 00:07:12   \n",
      "1497  Hello there, sorry if it is frustrating and ju... 2021-07-19 01:19:18   \n",
      "1498                                                NaN                 NaT   \n",
      "1499                                                NaN                 NaT   \n",
      "\n",
      "     Sentiment              Sub Category    Sub Category_test  \n",
      "0     Negative                       NaN  bug_picture_quality  \n",
      "1     Positive           extremely_happy                  NaN  \n",
      "2     Negative  saving_downloading_issue     bug_saving_files  \n",
      "3     Negative                    others        bug_app_crash  \n",
      "4     Negative                    others            bug_other  \n",
      "...        ...                       ...                  ...  \n",
      "1495  Positive           extremely_happy                  NaN  \n",
      "1496  Negative                    others                  NaN  \n",
      "1497  Negative                    others                  NaN  \n",
      "1498  Positive           extremely_happy                  NaN  \n",
      "1499  Positive           extremely_happy                  NaN  \n",
      "\n",
      "[1500 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#Loading the data: \n",
    "CustomerFeed = 'Canva_reviews.xlsx'\n",
    "df = pd.read_excel(CustomerFeed)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a16123-c29e-4ca6-9ae3-6cdfe1241985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overall it's really an amazing app. I've been ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey! Yes I gave a 5 star rating... coz I belie...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canva used to be a good app! But recently I've...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a brilliant app, but I have just one prob...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This was such a great app. I used to make BTS ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review Sentiment\n",
       "0  Overall it's really an amazing app. I've been ...  Negative\n",
       "1  Hey! Yes I gave a 5 star rating... coz I belie...  Positive\n",
       "2  Canva used to be a good app! But recently I've...  Negative\n",
       "3  It's a brilliant app, but I have just one prob...  Negative\n",
       "4  This was such a great app. I used to make BTS ...  Negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"review\", \"Sentiment\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6439bc2b-efc8-4b41-9c46-d5b1544c52dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = df[\"review\"] \n",
    "reviews.head()\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6beeb-94e4-4764-b2a3-49df47979ccf",
   "metadata": {},
   "source": [
    "Let's work on the reviews to simplify the input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8600c1ac-ba10-4a06-a79b-b79dd652db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_txt(review):\n",
    "    \"\"\"\n",
    "    This function receives a text and returns it edited as follows: \n",
    "    1, all words converted to lower case \n",
    "    2, integers removed\n",
    "    3, tokenize the words \n",
    "    4, punctuation removed \n",
    "    5, common words that are unnecessary are removed. \n",
    "    \"\"\"\n",
    "    \n",
    "    review_edited = []\n",
    "\n",
    "    #Converting to lower case: \n",
    "    review_edited = review.lower() \n",
    "    \n",
    "    #Removing integers: \n",
    "    pattern = r'[0-9]'\n",
    "    # Match all digits in the string and replace them with an empty string\n",
    "    review_edited = re.sub(pattern, '', review_edited) \n",
    "\n",
    "    #Tokenize the comment: \n",
    "    review_edited = word_tokenize(review_edited) \n",
    "\n",
    "    #Removing punctuation \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    review_edited = [''.join(tokenizer.tokenize(word)) for word in review_edited if len(tokenizer.tokenize(word))>0]\n",
    "\n",
    "    #Removing common words: \n",
    "    remove_list = stopwords.words('english') \n",
    "    to_remove = [ \"not\",'don',\"don't\",'should',\"should've\", 'ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",'mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn', \"wouldn't\"]\n",
    " \n",
    "    review_edited = [word for word in review_edited if not word in remove_list]\n",
    "    return(review_edited) \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "245822f2-e069-4b29-be78-181248920a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unable', 'save', 'work', 'nothing', 'works']\n",
      "Unable to save my work. Nothing works :(\n"
     ]
    }
   ],
   "source": [
    "reviews_edited = [edit_txt(review) for review in reviews]\n",
    "print(reviews_edited[13])\n",
    "print(reviews[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9f60c4-5713-463f-8352-9fc70a07a081",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['unabl', 'save', 'work', 'noth', 'work']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5 optional:find the stem of every word so that words like working, work, and worked are inputted equivantly in the model \n",
    "stem_token = PorterStemmer()\n",
    "print(stem_token.stem(\"working\"))\n",
    "\n",
    "reviews_edited = [[stem_token.stem(word) for word in review] for review in reviews_edited]\n",
    "reviews_edited[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb554da6-20a1-4fcf-aaf6-ceb33f343ee5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un', 'sav', 'work', 'noth', 'work']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "reviews_edited = [[lancaster.stem(word) for word in review] for review in reviews_edited]\n",
    "reviews_edited[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62e03455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having troubles downloading the files recently, even after cleaning up the cache. it just stuck at the downloading page (\"your so called making magic happens window\"). I even went to the extend of uninstall and re-install the app and still the same. just recently this problem occurs, before this it was just alright. for a premium user, if this problem still hasn't been improved, it's disappointing. might as well just discontinue the membership.\n",
      "['troubl', 'download', 'fil', 'rec', 'ev', 'cle', 'cach', 'stuck', 'download', 'pag', 'cal', 'mak', 'mag', 'hap', 'window', 'ev', 'went', 'extend', 'uninst', 'reinst', 'ap', 'stil', 'rec', 'problem', 'occ', 'alright', 'prem', 'us', 'problem', 'stil', 'nt', 'improv', 'disappoint', 'might', 'wel', 'discontinu', 'memb']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffed = df.iloc[:,0]\n",
    "x = reviews\n",
    "dfrank = df.iloc[:,1]\n",
    "\n",
    "#print(x[10])\n",
    "print(reviews[11])\n",
    "print(reviews_edited[11])\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34caa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative', 'Positive']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[\"Sentiment\"].tolist()\n",
    "ranking = np.unique(y)\n",
    "ranking = ranking.tolist()\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0394e7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'aap', 'abl', 'absolv', 'acc', 'acceiv', 'access', 'accid', 'accod']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the dictionary: \n",
    "lenx = len(x)\n",
    "\n",
    "Split = [] \n",
    "Dic = []\n",
    "dictionary = np.unique([word for review in reviews_edited for word in review]).tolist()\n",
    "dictionary[1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aec8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us try a different word embedding; this time we will use the glove methond and an online dictionary: \n",
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b796bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  list(embeddings_dict.keys())\n",
    "vectors = [embeddings_dict[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e63ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the dataset into 75% training set and 25% test set: \n",
    "x = x.to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, \n",
    "                                   random_state=104,  \n",
    "                                   test_size=0.25,  \n",
    "                                   shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7805b9ba-7f66-4916-93cb-ab2f371015da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05a58f44-01cf-4bf5-8a2e-cebc095c9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [edit_txt(comment) for comment in X_train]\n",
    "X_test = [edit_txt(comment) for comment in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b973a7c-cc99-41ad-99d2-fa1e178438d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c4ef9",
   "metadata": {},
   "source": [
    "**Update**\n",
    "\n",
    "- a global dictionary is used instead of our own dictionary\n",
    "\n",
    "       -> new customer feedbacks can also be evaluated. \n",
    "       -> the new word embedding in place can recognize word similarities \n",
    "       -> avoid large sparse one-hot vectors for the sake of memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f6b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as', 'it', 'by', 'at', '(', ')', 'from', 'his', \"''\", '``', 'an', 'be', 'has', 'are', 'have', 'but', 'were', 'not', 'this', 'who', 'they']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(words[0:40])\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2650b99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c251d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14c04e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict[\".\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1188d",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0845ea2-56b4-45d8-9ffb-d42ccc712466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to redefine the function so that if it has multiple sentences, it will run the below but it there's only one, it will \n",
    "#take an alternative approach. \n",
    "\n",
    "def gvec_input(x): \n",
    "    \"\"\"\n",
    "    This function takes any input, x, and returns a glove vector based on the \n",
    "    words introduced in the vocabulary (400,000 words). This function returns k vectors where k is the number of words in the \n",
    "    sentence. Every vector corresponds to a word in the dictionary and each entry will describe a feature of the word. \n",
    "    \n",
    "    inputs: \n",
    "    \n",
    "    x (string) : a statement from customers. \n",
    "    \n",
    "    outputs: \n",
    "    v (m,n)    : where m is the number of words in the sentence and n = 50 is the number of total features describing a word. \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    m = 50 #size of the standard input: \n",
    "    gv = np.zeros((n,m, 50))\n",
    "    for i in range(0, n): #looping over each comment \n",
    "        txt = x[i]\n",
    "        g = len(txt)\n",
    "        for l in range(g): #looping over each word \n",
    "            #if a word is not is the list of Glove embeddings, then assign an array which is the average of all embeddings: \n",
    "            if txt[l] not in words: \n",
    "                gv[i,l,:] = np.mean(vectors, axis = 0)\n",
    "            else: \n",
    "                gv[i,l,:] = embeddings_dict[txt[l]]\n",
    "        for h in range(g,m): \n",
    "            gv[i,h,:] = np.ones(50)\n",
    "    return(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f434725-fc73-4aca-882d-577adb7467f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting x_train and x_test to word embeddings: \n",
    "X_trainmod = gvec_input(X_train)\n",
    "X_testmod = gvec_input(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "634a487d-5031-420f-a656-6471bfc14228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(N): \n",
    "    N = N.astype(np.float64)\n",
    "    N = N.tolist()\n",
    "    n = len(N) \n",
    "    k = len(N[0]) \n",
    "    res = np.zeros((n,k)) \n",
    "    #looping over each sample: \n",
    "    for i in range(0,n): \n",
    "        #N[i] = N[i] - np.max[N[i]]\n",
    "        Sum = sum(np.exp(N[i])) \n",
    "        for j in range(k): \n",
    "            res[i,j] = np.exp(N[i][j])/Sum \n",
    "    return(res) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecbdbc-8f33-4214-97d6-9af88e7d22fe",
   "metadata": {},
   "source": [
    "**LSTM Model**\n",
    "\n",
    "Note : also apply to the time series dataset. then apply the tenserflow version and see if there is any difference with the simple RNN model. Maybe the simple RNN was just not able to pick up the patterns (which is not feasible since the RNN made by us were working at least with 60% accuracy) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6d73ffe-522d-40db-bd21-518ff207e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters: \n",
    "parameters = {} \n",
    "\n",
    "#dimension of every hidden neuron  \n",
    "n = vectors[0].shape[0] #size of embeddings \n",
    "m = 10                  #every hidden neuron has the same dim as the input (1*m) m = 6\n",
    "b = len(ranking)        #categories of the output \n",
    "z = 10                  #number of units for the cell state \n",
    "g = X_trainmod.shape[0] #number of samples \n",
    "#Initialize weights: \n",
    "parameters[\"forget\"] = np.random.randn(m + n, z)\n",
    "parameters[\"update\"] = np.random.randn(m + n, z)\n",
    "parameters[\"ihidden\"] = np.random.randn(m + n, z)\n",
    "parameters[\"candid\"] = np.random.randn(m + n, z)\n",
    "parameters[\"output_weights\"] = np.random.randn(m,b)\n",
    "\n",
    "#Initializing biases: \n",
    "parameters[\"b_f\"] = np.random.randn(z)\n",
    "parameters[\"b_u\"] = np.random.randn(z)\n",
    "parameters[\"b_i\"] = np.random.randn(z) #?\n",
    "parameters[\"b_c\"] = np.random.randn(z)\n",
    "parameters[\"b_y\"] = np.random.randn(b)\n",
    "\n",
    "#initial hidden and cell states \n",
    "parameters[\"a_prev\"] = np.tile(np.random.randn(1,m), (g,1)).reshape(g,1,m) #this make a copy of a_prev so we initiate all hidden states for all samples at the same time \n",
    "parameters[\"cell_prev\"] = np.tile(np.random.randn(1,z),(g,1)).reshape(g,1,z)\n",
    "\n",
    "cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb48e559-682c-4dcb-a4c1-65e10f53b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(xt, parameters):\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    sample_size = xt.shape[0]\n",
    "    \n",
    "    #Extract parameters: \n",
    "    w_f = parameters[\"forget\"]\n",
    "    w_u = parameters[\"update\"]\n",
    "    w_i = parameters[\"ihidden\"]\n",
    "    w_c = parameters[\"candid\"]\n",
    "    w_y = parameters[\"output_weights\"]\n",
    "    \n",
    "    b_f = parameters[\"b_f\"]\n",
    "    b_u = parameters[\"b_u\"]\n",
    "    b_i = parameters[\"b_i\"]\n",
    "    b_c = parameters[\"b_c\"]\n",
    "    b_y = parameters[\"b_y\"]\n",
    "\n",
    "    a_prev = parameters[\"a_prev\"][:,-1,:].reshape(sample_size, 1, 10)\n",
    "    cell_prev = parameters[\"cell_prev\"][:,-1,:].reshape(sample_size, 1, 10)\n",
    "\n",
    "    cache = ()\n",
    "    #concatenate the input and the hidden layer: \n",
    "    conc_res = np.concatenate((xt,a_prev.reshape(sample_size,a_prev.shape[2])), axis = 1)\n",
    "\n",
    "    #Calculating the gates and save into the cache: \n",
    "\n",
    "    #forget gate: \n",
    "    forget = Softmax(np.dot(conc_res,w_f) + b_f)\n",
    "    #update gate: \n",
    "    update = Softmax(np.dot(conc_res,w_u) + b_u)\n",
    "    #candidate cell state: \n",
    "    candid = np.tanh(np.dot(conc_res, w_c) + b_c)\n",
    "    #i gate: \n",
    "    ihidden = Softmax(np.dot(conc_res, w_i) + b_i)\n",
    "    \n",
    "    #Updating the new cell state and parameter cell_next for next step: \n",
    "    cell_next = (forget * cell_prev[:,-1,:] + update * candid).reshape(sample_size, 1, cell_prev.shape[2])\n",
    "    parameters[\"cell_prev\"]= np.concatenate((parameters[\"cell_prev\"],cell_next), axis = 1)\n",
    "    \n",
    "    #Updating the new hidden state and parameter a_prev for next step: \n",
    "    a_next = (np.tanh(cell_next[:,-1,:]) * ihidden).reshape(sample_size, 1, a_prev.shape[2])\n",
    "    parameters[\"a_prev\"] = np.concatenate((parameters[\"a_prev\"],a_next), axis = 1)\n",
    "    \n",
    "    #saving all the parameters: \n",
    "    #cache = {\"forget\" : forget, \"update\" : update, \"candid\" : candid, \"ihidden\" : ihidden}\n",
    "    cache = (forget, update, candid, ihidden)\n",
    "    \n",
    "    return(a_next,cell_next, cache, parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eca69258-0eef-40cc-9c94-f53d89d78774",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_next,cell_next, cache, parameters = lstm_cell(X_trainmod[:,0,:], parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2bb395d-2aea-4b36-9e5f-a59910a247d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 2, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[\"a_prev\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bb05e86-bad4-42a9-ac0f-64fb8e34d423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[0].shape #this is the forget gate calculated for all 1125 training samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3123aec9-5fe5-4581-979c-f9c106f8ac81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.15756682e-04, 3.65098893e-07, 1.80765761e-01, 4.85120299e-05,\n",
       "       8.04847614e-01, 2.48871153e-03, 3.52073548e-06, 7.95372747e-07,\n",
       "       1.13287986e-02, 1.65400580e-07])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[1][0] #this is the update gate for the first sample in the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef83c3ad-a865-450d-814f-75ea0b9fd71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99999986, -0.99739466,  0.90833275,  0.99999972,  1.        ,\n",
       "       -0.99999982,  0.83290237, -0.99825465, -0.99999767, -0.80048775])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the first candidate cell_state: \n",
    "cache[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98480f96-69f5-4018-bfc7-5f6835b16450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 1, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_next.shape #this is the cell_state for the second element in the sequence has 10 neurons in it. all 1125 samples are included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86cbbf8a-4ff0-44ce-821d-4b57fb492c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[3].shape #this is the i gate output for updating the hidden state at each time point. has 10 enteries corresponding to the 10 neurons of \n",
    "#the hidden state and all samples are included. now the problem is that for each step you change the parameters individually? shouldn't you add the \n",
    "#change required for each one of them and update the parameters simultanously for all of them? yes. that's right otherwise the parameters are \n",
    "#updated individually and the machine never learns from the collection of all samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ffdbc98-f5cf-4032-89f4-94f7931dd868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 1, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_next.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04efb6-4d71-4435-a431-a19b62edfd0d",
   "metadata": {},
   "source": [
    "So far, we have designed the lstm_cell that performs a single step in LSTM forward calculations for time t. The next  step would be to figure out what parameters we want to be outputed by the lstm_step and then run this for all time steps and finally make a precition based on a_next. \n",
    "\n",
    "**also remember to go back at the rnn back propagation step_wise you would need to add a_next to the calculations which you've missed before.**\n",
    "if we need to back propagate through time remember to sum all the change required for forget gate together after the calculation of gradients. \n",
    "\n",
    "figure out why b is summed over all elements for the gradient. \n",
    "\n",
    "could it possibly be problematic if we have too much of padding for the end of the sentences? because imagine the result of every gate would be the same and then would it make a difference of would it just confuse the machine further and make it less attentive to the beginning of the sequence? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f03fa56c-15ad-427d-8ce6-bd65573ae708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(xt, parameters): \n",
    "    \n",
    "    \"\"\"\n",
    "    this step will repeat the procedure of lstm_cell for all time steps \n",
    "    note that in order to avoid miscalculations, do not run the rnn stepward before the lstm_forward. \n",
    "    this is because the parameters are being updated while running the lstm stepwise and then starting the lstm forward just appends \n",
    "    to the resutls instead of starting over. \n",
    "\n",
    "    within this function the lstm forward cell will be run for the length of sequences xt. cell state and hidden state are initialized \n",
    "    randomly. In each iteration, the cell state and hidden state will be calculated; these values will be appended to parameters and outputed \n",
    "    after each lstm step. \n",
    "\n",
    "    a_next and cell_next will also be outputed so that the prediction can be easily made based on the last hidden state. \n",
    "\n",
    "    The cache will keep track of every gate output and the candidate cell state and then all of the outputs will be appended to the caches to \n",
    "    keep track of all outputs. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n = xt.shape[1] #length of each comment \n",
    "    w_y = parameters[\"output_weights\"]\n",
    "    b_y = parameters[\"b_y\"]\n",
    "    m = w_y.shape[1]\n",
    "    j = xt.shape[0]\n",
    "    \n",
    "    \n",
    "    caches= [] #for keeping track of all caches \n",
    "    \n",
    "    for i in range(n): \n",
    "        \n",
    "        \n",
    "        if i == n-1: #making prediction on the last step\n",
    "            print(f\"end of process\")\n",
    "            a_next, cell_next, cache, parameters = lstm_cell(xt[:,i,:], parameters)\n",
    "            caches.append(cache)\n",
    "            y_pred = Softmax((np.dot(a_next, w_y) + b_y).reshape(j,m))\n",
    "            \n",
    "        else:    \n",
    "            a_next, cell_next, cache, parameters = lstm_cell(xt[:,i,:], parameters)\n",
    "            caches.append(cache)\n",
    "\n",
    "    \n",
    "    return(y_pred, caches, parameters) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d15a41b4-9544-4061-9c5c-12e84c380efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of process\n"
     ]
    }
   ],
   "source": [
    "y_pred, caches, parameters = lstm_forward(X_trainmod,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8face0a7-34f5-4c87-92d2-47101bc94815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7db3dcfd-e30e-4701-85a2-73fec1831267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.15437574e-08, 4.05903329e-03, 1.57028598e-04, ...,\n",
       "         1.25474824e-05, 3.97849926e-01, 1.63915839e-06],\n",
       "        [5.73178039e-08, 1.41823358e-02, 4.48376642e-08, ...,\n",
       "         1.93183237e-08, 3.74687926e-02, 9.08275216e-07],\n",
       "        [3.87949986e-05, 3.90007152e-01, 2.28109524e-06, ...,\n",
       "         2.17214737e-09, 6.07115436e-01, 6.98439213e-04],\n",
       "        ...,\n",
       "        [3.30460352e-09, 5.94545360e-03, 4.11256752e-10, ...,\n",
       "         3.70156483e-09, 7.15066544e-01, 3.67323766e-07],\n",
       "        [3.30460352e-09, 5.94545360e-03, 4.11256752e-10, ...,\n",
       "         3.70156483e-09, 7.15066544e-01, 3.67323766e-07],\n",
       "        [2.71721088e-07, 6.90130285e-01, 3.96879873e-08, ...,\n",
       "         1.28599502e-07, 1.09097046e-01, 1.14652795e-07]]),\n",
       " array([[6.05034879e-05, 1.52794506e-06, 1.37058405e-02, ...,\n",
       "         1.47766648e-04, 9.99416966e-02, 1.48149391e-03],\n",
       "        [1.51158298e-05, 7.91720469e-06, 3.14743653e-01, ...,\n",
       "         2.12769750e-02, 1.90750979e-04, 8.77638263e-04],\n",
       "        [1.66622017e-02, 9.96928890e-03, 6.60985022e-01, ...,\n",
       "         2.11745659e-04, 4.91380533e-03, 1.65554641e-09],\n",
       "        ...,\n",
       "        [1.25743849e-03, 1.83755294e-06, 5.51183482e-03, ...,\n",
       "         3.25186881e-01, 3.47333629e-03, 1.24160377e-02],\n",
       "        [1.25743849e-03, 1.83755294e-06, 5.51183482e-03, ...,\n",
       "         3.25186881e-01, 3.47333629e-03, 1.24160377e-02],\n",
       "        [9.30336206e-04, 8.32737923e-07, 3.22170115e-02, ...,\n",
       "         1.08119668e-01, 5.97419138e-02, 1.29811258e-03]]),\n",
       " array([[ 0.99953215, -0.9958955 ,  0.99960581, ..., -0.06696279,\n",
       "         -0.99999186, -0.99970095],\n",
       "        [ 0.99999451, -0.38782294,  0.99999997, ...,  0.89671812,\n",
       "         -0.59744103,  0.74865433],\n",
       "        [ 1.        ,  0.9947783 ,  0.9998467 , ..., -0.99991524,\n",
       "          0.11239295,  0.6847846 ],\n",
       "        ...,\n",
       "        [ 0.99976552, -0.79038213,  0.99999921, ..., -0.99982714,\n",
       "         -0.98542882, -0.09317888],\n",
       "        [ 0.99976552, -0.79038213,  0.99999921, ..., -0.99982714,\n",
       "         -0.98542882, -0.09317888],\n",
       "        [ 0.99999973, -0.98304979,  1.        , ..., -0.78243972,\n",
       "         -0.9842782 ,  0.94674217]]),\n",
       " array([[3.19503140e-08, 2.33731565e-04, 3.98882949e-06, ...,\n",
       "         8.86890417e-07, 8.71354708e-03, 8.75699691e-02],\n",
       "        [1.95696292e-07, 2.10454905e-04, 1.47990072e-04, ...,\n",
       "         1.16498038e-02, 1.95270914e-03, 7.96200981e-02],\n",
       "        [4.21087423e-02, 1.88360191e-01, 2.87718131e-05, ...,\n",
       "         4.04060279e-01, 1.10657908e-03, 1.28705878e-01],\n",
       "        ...,\n",
       "        [4.89893265e-08, 4.81486139e-04, 1.41853939e-04, ...,\n",
       "         9.94200407e-07, 5.02348281e-06, 2.00448522e-04],\n",
       "        [4.89893265e-08, 4.81486139e-04, 1.41853939e-04, ...,\n",
       "         9.94200407e-07, 5.02348281e-06, 2.00448522e-04],\n",
       "        [1.52702295e-08, 5.18636507e-06, 1.95551588e-07, ...,\n",
       "         1.26547930e-06, 2.70622968e-06, 2.38349566e-02]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches[0]#this is all the output of gates and cadidate gate for the first word in the sequence. there are 50 caches appended to this var.\n",
    "#the parameters are forget, update, candid, ihidden respectively. how many neurons in each forget gate? must be 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8073401-b249-4239-9b4c-7e347a34fae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches[0][0].shape #10 neurons in the forget gate. similarly, you can check the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4709bf6-56c0-45f3-814e-68574a9efb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50939109, 0.49060891],\n",
       "       [0.50938971, 0.49061029],\n",
       "       [0.50939092, 0.49060908],\n",
       "       ...,\n",
       "       [0.50939115, 0.49060885],\n",
       "       [0.50938962, 0.49061038],\n",
       "       [0.50939114, 0.49060886]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f595f5-a048-4e0d-ac86-33e413f924f6",
   "metadata": {},
   "source": [
    "two problems: one is that the lstm might actually have a problem such that all the outputs are predicted to be the same. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5c16e-6e30-47c8-8b8c-b4a0cd53e9cd",
   "metadata": {},
   "source": [
    "The problem is that the a_next and cell_state are dimineshed in value significantly due to the Softmax activation functions used. \n",
    "therefore, when we arrive at the last step of the sequences, a_next multiplied by w_y will cause its value to approach to zero. the Softmax is then applied only to the b_y which is why the output values are the same. \n",
    "\n",
    "how about normalizing the values? in softmax activation function? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26d41c68-2618-49cf-ab0f-d6ab2d1bd880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.32939460e-01, -3.69897842e-07,  1.29179079e-05, ...,\n",
       "        -4.37767020e-07,  9.10756521e-05,  6.74996842e-09],\n",
       "       [-7.32939725e-01,  2.96008992e-05,  1.29176573e-05, ...,\n",
       "        -4.37760185e-07,  9.10737995e-05,  6.74995992e-09],\n",
       "       [-7.32939492e-01,  3.26812573e-06,  1.29178774e-05, ...,\n",
       "        -4.37766190e-07,  9.10754272e-05,  6.74996738e-09],\n",
       "       ...,\n",
       "       [-7.32939448e-01, -1.69916898e-06,  1.29179190e-05, ...,\n",
       "        -4.37767323e-07,  9.10757342e-05,  6.74996879e-09],\n",
       "       [-7.32939741e-01,  3.14964969e-05,  1.29176415e-05, ...,\n",
       "        -4.37759753e-07,  9.10736824e-05,  6.74995938e-09],\n",
       "       [-7.32939449e-01, -1.52075582e-06,  1.29179175e-05, ...,\n",
       "        -4.37767282e-07,  9.10757232e-05,  6.74996874e-09]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[\"cell_prev\"][:,-1,:] #values of the last cell state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2a247de-c31a-4f91-b48c-8973fef5bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.48269440e-05, -1.03846391e-07,  4.05658195e-07, ...,\n",
       "        -1.04809835e-09,  5.26497050e-05,  5.31124396e-10],\n",
       "       [-3.48271917e-05,  8.31011921e-06,  4.05654785e-07, ...,\n",
       "        -1.04808228e-09,  5.26490262e-05,  5.31124099e-10],\n",
       "       [-3.48269740e-05,  9.17502987e-07,  4.05657781e-07, ...,\n",
       "        -1.04809640e-09,  5.26496226e-05,  5.31124360e-10],\n",
       "       ...,\n",
       "       [-3.48269330e-05, -4.77030892e-07,  4.05658347e-07, ...,\n",
       "        -1.04809906e-09,  5.26497351e-05,  5.31124409e-10],\n",
       "       [-3.48272074e-05,  8.84227759e-06,  4.05654569e-07, ...,\n",
       "        -1.04808126e-09,  5.26489833e-05,  5.31124080e-10],\n",
       "       [-3.48269344e-05, -4.26942489e-07,  4.05658326e-07, ...,\n",
       "        -1.04809897e-09,  5.26497310e-05,  5.31124407e-10]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[\"a_prev\"][:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29a33286-f138-4d7e-a18e-b8b091f2eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reminder to correct the back propagation of the last programmed rnn_forward code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dda9ca40-7676-4145-8003-2d02b723d37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainmod = np.zeros((1125,2))\n",
    "for i in range(len(y_train)): \n",
    "    y_trainmod[i, :] = vec_output(y_train[i])\n",
    "y_trainmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b631dff7-7e8c-436c-8931-64368a1879f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 52, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[\"cell_prev\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "687c2e32-0c62-4759-97b5-7058b8aa1531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative', 'Positive']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ad10b33-4809-4f9e-9861-b456d1e8bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_lstm(X_trainmod, parameters, y_train, iterations, alpha): \n",
    "    \"\"\"\n",
    "    here we will run one step of the back propagation process. \n",
    "\n",
    "    note that cell_prev has 51 columns including the initialized cell state. \n",
    "    \"\"\"\n",
    "    #first make a prediction based on the past? or maybe what it can receive is all the parameters after the lstm_forward step? coz it already \n",
    "    #has it all? so what should we do? we take all the parameters as input. in this step you should already have run the lstm forward. \n",
    "    #then we do what? take the lstm forward outputs and unwrap cache for gates outputs and parameters for the weights, biases, and the a_prev \n",
    "    #and cell_prev. now in this case, you can run a reversed i iteration so that if i == last iteration, then we take the difference between the \n",
    "    #prediction and the y_actual. then we move thrgouh the iterations back propagating this difference to update all parameters to reflect this change. \n",
    "    \n",
    "    n, T_x ,emb = X_trainmod.shape\n",
    "    p_y = 2                   #number of categories (negative vs positive ->2) \n",
    "    \n",
    "    #define the y_actual: \n",
    "    y_trainmod = np.zeros((n,p_y))\n",
    "    \n",
    "    for i in range(n): \n",
    "        \n",
    "        y_trainmod[i, :] = vec_output(y_train[i])\n",
    "\n",
    "    #unpack the parameters: \n",
    "    w_f = parameters[\"forget\"]\n",
    "    w_u = parameters[\"update\"]\n",
    "    w_i = parameters[\"ihidden\"]\n",
    "    w_c = parameters[\"candid\"]\n",
    "    w_y = parameters[\"output_weights\"]\n",
    "    \n",
    "    b_f = parameters[\"b_f\"]\n",
    "    b_u = parameters[\"b_u\"]\n",
    "    b_i = parameters[\"b_i\"]\n",
    "    b_c = parameters[\"b_c\"]\n",
    "    b_y = parameters[\"b_y\"]\n",
    "\n",
    "   \n",
    "\n",
    "    #store the number of hidden neurons: \n",
    "    n_a = w_y.shape[0]\n",
    "\n",
    "    #start the iterations: \n",
    "    for j in range(iterations):\n",
    "        \n",
    "        #make a prediction: \n",
    "        y_pred, caches, parameters = lstm_forward(X_trainmod, parameters)#this line can be problematic if the lstm_forward is previously run. \n",
    "\n",
    "        #update the hidden and cell states: \n",
    "        a_state = parameters[\"a_prev\"]\n",
    "        cell_state = parameters[\"cell_prev\"]\n",
    "        \n",
    "        #calculate the loss function: \n",
    "        cost = y_trainmod - y_pred\n",
    "        \n",
    "        dht = np.dot(cost, w_y.T) \n",
    "        dwy = np.dot(dht.T,cost)\n",
    "        dby = np.sum(cost, axis = 1, keepdims = True)\n",
    "        dc_next,dh_next = 0,0 \n",
    "        dwf, dwu, dwihidden, dwcandid = 0,0,0,0\n",
    "        dbcandid, dbf, dbu, dbihidden = 0,0,0,0\n",
    "        \n",
    "        for i in reversed(range(T_x)): \n",
    "            #load your cache: \n",
    "            ft, ut, candidt, ihiddent = caches[i] \n",
    "            #we take the 50th element of cell state (which is actually the last one) \n",
    "            dht = dht + dh_next \n",
    "            dct = dht * ihiddent * (1 - (np.tanh(cell_state[:,i+1,:]))**2) + dc_next \n",
    "            dcandidt = dct * ut * (1-candidt**2)\n",
    "\n",
    "            #gate derivatives: \n",
    "            dihiddent = dht * np.tanh(cell_state[:,i+1, :]) * ihiddent * (1-ihiddent)\n",
    "            dft = dct * cell_state[:,i, :] * ft * (1-ft)\n",
    "            dut = dct * candidt * ut * (1-ut)\n",
    "        \n",
    "            #update the weights: \n",
    "            dwf += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dft) #change dims to vars. \n",
    "            dwu += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dut)\n",
    "            dwihidden += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dihiddent)\n",
    "            dwcandid += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dcandidt)\n",
    "\n",
    "            dbcandid += np.sum(dcandidt, axis=1, keepdims=True) #add all the rows together. (1125 by 3 collapsed to 1 by 3) \n",
    "            dbf += np.sum(dft, axis=1, keepdims=True) \n",
    "            dbu += np.sum(dut, axis=1, keepdims=True) \n",
    "            \n",
    "            dbihidden += np.sum(dihiddent, axis=1, keepdims=True) \n",
    "            dh_next = np.dot(dft, w_f[-10:, :].T) + np.dot(dcandidt, w_c[-10:, :].T) + np.dot(dihiddent, w_i[-10:, :].T) + np.dot(dut, w_u[-10:, :].T)\n",
    "            dc_next = dct * ft \n",
    "\n",
    "        #update the parameters: \n",
    "        w_f = w_f - alpha * dwf\n",
    "        w_u = w_u -alpha * dwu\n",
    "        w_i = w_i - alpha * dwihidden\n",
    "        w_c = w_c - alpha * dwcandid \n",
    "        w_y = w_y -alpha * dwy \n",
    "        \n",
    "        b_f = b_f - alpha * dbf \n",
    "        b_u = b_u - alpha * dbu \n",
    "        b_c = b_c - alpha * dbcandid\n",
    "        b_i = b_i - alpha * dbihidden\n",
    "        b_y = b_y - alpha * dby \n",
    "\n",
    "        #Re-initialize the variables a_prev and cell_prev for the next run: \n",
    "        a_prev = np.tile(np.random.randn(1,n_a), (n,1)).reshape(n,1,n_a) #this make a copy of a_prev so we initiate all hidden states for all samples at the same time \n",
    "        cell_prev = np.tile(np.random.randn(1,n_a),(n,1)).reshape(n,1,n_a)\n",
    "\n",
    "        #Delete the content of the caches: \n",
    "        caches = []\n",
    "\n",
    "        #Save the updated parameters: \n",
    "        parameters = {\n",
    "            \"forget\": w_f, \"update\": w_u, \"ihidden\": w_i, \"candid\": w_c, \"output_weights\": w_y, \"b_f\": b_f, \"b_u\": b_u, \"b_i\":b_i, \"b_c\":b_c, \"b_y\":b_y, \"a_prev\": a_prev, \"cell_prev\": cell_prev}\n",
    "    \n",
    "\n",
    "    return(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a699c012-2974-486d-b414-d7bc5d43520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of process\n"
     ]
    }
   ],
   "source": [
    "parameters = backprop_lstm(X_trainmod, parameters, y_train ,1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8679e7a-10f4-443e-b86b-8016dd2b91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters: \n",
    "parameters = {} \n",
    "\n",
    "#dimension of every hidden neuron  \n",
    "n = vectors[0].shape[0] #size of embeddings \n",
    "m = 10                  #every hidden neuron has the same dim as the input (1*m) m = 6\n",
    "b = len(ranking)        #categories of the output \n",
    "z = 10                  #number of units for the cell state \n",
    "g = X_trainmod.shape[0] #number of samples \n",
    "#Initialize weights: \n",
    "parameters[\"forget\"] = np.random.randn(m + n, z)\n",
    "parameters[\"update\"] = np.random.randn(m + n, z)\n",
    "parameters[\"ihidden\"] = np.random.randn(m + n, z)\n",
    "parameters[\"candid\"] = np.random.randn(m + n, z)\n",
    "parameters[\"output_weights\"] = np.random.randn(m,b)\n",
    "\n",
    "#Initializing biases: \n",
    "parameters[\"b_f\"] = np.random.randn(z)\n",
    "parameters[\"b_u\"] = np.random.randn(z)\n",
    "parameters[\"b_i\"] = np.random.randn(z) #?\n",
    "parameters[\"b_c\"] = np.random.randn(z)\n",
    "parameters[\"b_y\"] = np.random.randn(b)\n",
    "\n",
    "#initial hidden and cell states \n",
    "parameters[\"a_prev\"] = np.tile(np.random.randn(1,m), (g,1)).reshape(g,1,m) #this make a copy of a_prev so we initiate all hidden states for all samples at the same time \n",
    "parameters[\"cell_prev\"] = np.tile(np.random.randn(1,z),(g,1)).reshape(g,1,z)\n",
    "\n",
    "cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64b973b3-e947-42a9-bdfd-0e48e29654a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m     y_trainmod[i, :] \u001b[38;5;241m=\u001b[39m vec_output(y_train[i])\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#store the number of hidden neurons: \u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m n_a \u001b[38;5;241m=\u001b[39m w_y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#start the iterations: \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m     30\u001b[0m         \n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m#make a prediction: \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w_y' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "iterations = 1\n",
    "  \n",
    "#first make a prediction based on the past? or maybe what it can receive is all the parameters after the lstm_forward step? coz it already \n",
    "#has it all? so what should we do? we take all the parameters as input. in this step you should already have run the lstm forward. \n",
    "#then we do what? take the lstm forward outputs and unwrap cache for gates outputs and parameters for the weights, biases, and the a_prev \n",
    "#and cell_prev. now in this case, you can run a reversed i iteration so that if i == last iteration, then we take the difference between the \n",
    "#prediction and the y_actual. then we move thrgouh the iterations back propagating this difference to update all parameters to reflect this change. \n",
    "    \n",
    "n, T_x ,emb = X_trainmod.shape\n",
    "p_y = 2                   #number of categories (negative vs positive ->2) \n",
    "    \n",
    "    #define the y_actual: \n",
    "y_trainmod = np.zeros((n,p_y))\n",
    "    \n",
    "for i in range(n): \n",
    "        \n",
    "    y_trainmod[i, :] = vec_output(y_train[i])\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #store the number of hidden neurons: \n",
    "n_a = w_y.shape[0]\n",
    "\n",
    "    #start the iterations: \n",
    "for j in range(iterations):\n",
    "        \n",
    "        #make a prediction: \n",
    "    y_pred, caches, parameters = lstm_forward(X_trainmod, parameters)#this line can be problematic if the lstm_forward is previously run. \n",
    "    #unpack the parameters:\n",
    "    w_f = parameters[\"forget\"]\n",
    "    w_u = parameters[\"update\"]\n",
    "    w_i = parameters[\"ihidden\"]\n",
    "    w_c = parameters[\"candid\"]\n",
    "    w_y = parameters[\"output_weights\"]\n",
    "    \n",
    "    b_f = parameters[\"b_f\"]\n",
    "    b_u = parameters[\"b_u\"]\n",
    "    b_i = parameters[\"b_i\"]\n",
    "    b_c = parameters[\"b_c\"]\n",
    "    b_y = parameters[\"b_y\"]\n",
    "    a_state = parameters[\"a_prev\"]\n",
    "    cell_state = parameters[\"cell_prev\"]\n",
    "        \n",
    "        #calculate the loss function: \n",
    "    cost = y_trainmod - y_pred\n",
    "        \n",
    "    dht = np.dot(cost, w_y.T) \n",
    "    dwy = np.dot(dht.T,cost)\n",
    "    dby = np.sum(cost, axis = 0, keepdims = True)\n",
    "    dc_next,dh_next = 0,0 \n",
    "    dwf, dwu, dwihidden, dwcandid = 0,0,0,0\n",
    "    dbcandid, dbf, dbu, dbihidden = 0,0,0,0\n",
    "        \n",
    "    for i in reversed(range(T_x)): \n",
    "            #load your cache: \n",
    "        ft, ut, candidt, ihiddent = caches[i] \n",
    "            #we take the 50th element of cell state (which is actually the last one) \n",
    "        dht = dht + dh_next \n",
    "        dct = dht * ihiddent * (1 - (np.tanh(cell_state[:,i+1,:]))**2) + dc_next \n",
    "        dcandidt = dct * ut * (1-candidt**2)\n",
    "\n",
    "            #gate derivatives: \n",
    "        dihiddent = dht * np.tanh(cell_state[:,i+1, :]) * ihiddent * (1-ihiddent)\n",
    "        dft = dct * cell_state[:,i, :] * ft * (1-ft)\n",
    "        dut = dct * candidt * ut * (1-ut)\n",
    "        \n",
    "            #update the weights: \n",
    "        dwf += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dft) #change dims to vars. \n",
    "        dwu += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dut)\n",
    "        dwihidden += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dihiddent)\n",
    "        dwcandid += np.dot(np.hstack([X_trainmod[:,i,:].reshape(n,emb),a_state[:,i,:]]).T,dcandidt)\n",
    "\n",
    "        dbcandid += np.sum(dcandidt, axis=0, keepdims=True) #add all the rows together. (1125 by 3 collapsed to 1 by 3) \n",
    "        dbf += np.sum(dft, axis = 0, keepdims = True) \n",
    "        dbu += np.sum(dut, axis=0, keepdims=True) \n",
    "            \n",
    "        dbihidden += np.sum(dihiddent, axis=0, keepdims=True) \n",
    "        dh_next = np.dot(dft, w_f[-10:, :].T) + np.dot(dcandidt, w_c[-10:, :].T) + np.dot(dihiddent, w_i[-10:, :].T) + np.dot(dut, w_u[-10:, :].T)\n",
    "        dc_next = dct * ft \n",
    "\n",
    "        #update the parameters: \n",
    "    w_f = w_f - alpha * dwf\n",
    "    w_u = w_u -alpha * dwu\n",
    "    w_i = w_i - alpha * dwihidden\n",
    "    w_c = w_c - alpha * dwcandid \n",
    "    w_y = w_y -alpha * dwy \n",
    "        \n",
    "    b_f = b_f - alpha * dbf \n",
    "    b_u = b_u - alpha * dbu \n",
    "    b_c = b_c - alpha * dbcandid\n",
    "    b_i = b_i - alpha * dbihidden\n",
    "    b_y = b_y - alpha * dby \n",
    "\n",
    "    #Re-initialize the variables a_prev and cell_prev for the next run: \n",
    "    a_prev = np.tile(np.random.randn(1,n_a), (n,1)).reshape(n,1,n_a) #this make a copy of a_prev so we initiate all hidden states for all samples at the same time \n",
    "    cell_prev = np.tile(np.random.randn(1,n_a),(n,1)).reshape(n,1,n_a)\n",
    "\n",
    "        #Delete the content of the caches: \n",
    "    caches = []\n",
    "\n",
    "        #Save the updated parameters: \n",
    "    parameters = {\n",
    "        \"forget\": w_f, \"update\": w_u, \"ihidden\": w_i, \"candid\": w_c, \"output_weights\": w_y, \n",
    "        \"b_f\": b_f, \"b_u\": b_u, \"b_i\":b_i, \"b_c\":b_c, \"b_y\":b_y, \n",
    "        \"a_prev\": a_prev, \"cell_prev\": cell_prev}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e88c3-9de0-49fd-8b6c-2a1795f10675",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, caches, parameters = lstm_forward(X_trainmod, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7954f15-0c4f-4334-8418-267edb471356",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
